# Kafka-Spark-Streaming Project ğŸš€

## ğŸ“Œ Overview
This project demonstrates a real-time data processing pipeline using **Kafka**, **Spark Streaming**, **Docker**, and **Grafana** for visualization. Messages are produced to Kafka topics, consumed by Spark Streaming workers, and then displayed in Grafana.

## ğŸ“ Project Structure
```
kafka-spark-streaming/
â”‚â”€â”€ docker-compose.yml        # Defines all services (Kafka, Zookeeper, Spark, Grafana)
â”‚â”€â”€ event.txt                 # Sample event messages in JSON format
â”‚â”€â”€ kafka_producer.py         # Python Kafka Producer script
â”‚â”€â”€ spark_streaming.py        # Spark Streaming Consumer script
â”‚â”€â”€ grafana_dashboard.json    # Grafana dashboard configuration
â”‚â”€â”€ README.md                 # Documentation
```

## âš™ï¸ Technologies Used
- **Apache Kafka** â†’ Message broker  
- **Apache Spark (PySpark)** â†’ Stream processing  
- **Docker & Docker-Compose** â†’ Containerized setup  
- **Grafana** â†’ Data visualization  
- **Python** â†’ Producer & Consumer scripts  

## ğŸ› ï¸ Setup and Installation
### 1ï¸âƒ£ Clone the Repository
```bash
git clone git@github.com:dinkleva/kafka-spark-streaming.git
cd kafka-spark-streaming
```

### 2ï¸âƒ£ Start the Dockerized Services
```bash
docker-compose up -d
```
This will start:
- **Kafka & Zookeeper**  
- **Spark Cluster (1 Master + 3 Workers)**  
- **Grafana for visualization**  

## ğŸ“ Producing Messages to Kafka
Edit `event.txt` with JSON messages and produce them:  
```bash
cat event.txt | docker exec -i kafka kafka-console-producer.sh --topic events --bootstrap-server kafka:9092
```
âœ… **Verify Production:**  
```bash
docker exec -it kafka kafka-console-consumer.sh --topic events --from-beginning --bootstrap-server kafka:9092
```

## ğŸ¯ Running Spark Streaming Consumer
To start Spark consumer:  
```bash
docker exec -it spark-master spark-submit --master spark://spark-master:7077 /opt/bitnami/spark/spark_streaming.py
```
âœ… **Output:** Processed messages from Kafka displayed in the console.  

## ğŸ“Š Visualizing Data in Grafana
- Open Grafana at: [http://localhost:3000](http://localhost:3000)  
- Default login:
  - **Username:** `admin`
  - **Password:** `admin`
- Configure **Spark streaming output** as a data source.  
- Import `grafana_dashboard.json`.  

## ğŸ› Troubleshooting
### Kafka is not producing messages
```bash
docker logs kafka
```
Check for **broker errors** in logs.

### Spark can't connect to Kafka
Check the network:
```bash
docker exec -it spark-master ping kafka
```
If unreachable, update `docker-compose.yml` with correct service names.

## ğŸ› ï¸ Optimizing Message Handling
- **Partitioning Kafka Topics** â†’ Distributes load across consumers  
- **Scaling Spark Executors** â†’ `docker-compose scale spark-worker=5`  
- **Using Kafka Consumer Groups** â†’ Multiple consumer workers  
- **Batching & Windowing in Spark** â†’ Optimized data processing  

## ğŸ“Œ Next Steps
- **ğŸ“¥ Push new features**
- **ğŸš€ Deploy on Kubernetes**
- **ğŸ“ˆ Monitor with Prometheus**
- **â³ Optimize message latency**

## ğŸ‘¨â€ğŸ’» Author
ğŸš€ **Dinkleva** - Passionate about **Data Engineering & Streaming Architectures**  
ğŸ“§ Contact: `dinkleva@example.com`

âœ… **Star this repo** â­ if you found it helpful!  
